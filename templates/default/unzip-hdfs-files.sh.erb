#!/bin/bash

set -e

help() {
  echo "usage: $0 localWorkdir srcFileInHdfs HDFS_USER"
  exit 1
}


if [ $# -ne 3 ] ; then
  help
fi

hdfsFile=$1
workdir=$2
HDFS_USER=$3

cd $workdir
freespace=`df $PWD | awk '/[0-9]%/{print $(NF-2)}'`
freespace=$(($freespace * 1000))

filesize=`/srv/hops/hadoop/bin/hadoop fs -du -s $hdfsFile | awk  '{print $1}'`
# Amount of space needed to download and decompress the file
filesize=$(($filesize * 3))

if [ $filesize -gt $freespace ] ; then
  echo "Not enough free space on the local harddisk"
  exit 2
fi  

file=$(basename $hdfsFile)
dir=$(dirname "${hdfsFile}")


#zips=$(hadoop fs -ls /yourpath/*.zip | awk '{print $8}')

${HADOOP_DIR}/bin/hdfs dfs -copyToLocal $hdfsfile $workdir

# State Machine State Change. File now staged to local FS.
# Java program checks if this file is written to see if this file exists:  $workdir/$(basename $hdfsfile)


# http://manpages.ubuntu.com/manpages/trusty/en/man1/dtrx.1.html
# , everything will be written to  a  dedicated  directory  that's named  after  the  archive.
# dtrx will also change the permissions to  ensure that the owner can read and write all those files.

dtrx -o -n $hdfsFile

# State Machine State Change. File now extracted to local FS.
# Java program checks if this file is written to see if this directory exists:  $workdir/$(basename $hdfsfile)


filedir="${file%%.*}"
stripped=${dir}/${filedir}

# case "$file" in 
#   *.tar.gz)
#      stripped=$(basename $hdfsFile .tar.gz)
#      mkdir -p $stripped
#      tar zxf $file -C ./$stripped
#     ;;
#   *.bz2)
#      stripped=$(basename $hdfsFile .bz2)
#      mkdir -p $stripped
#      tar jxf $file -C ./$stripped
#     ;;
#   *.bzip2)
#      stripped=$(basename $hdfsFile .bz2)
#      mkdir -p $stripped
#      tar jxf $file -C ./$stripped
#     ;;
#   *.tgz)
#      stripped=$(basename $hdfsFile .tgz)
#      mkdir -p $stripped
#      tar zxf $file -C ./$stripped
#     ;;
#   *.gz)
#      stripped=$(basename $hdfsFile .gz)
#      mkdir -p $stripped
#      gunzip -d -c $file > ./$stripped
#     ;;
#   *.7z)
#      stripped=$(basename $hdfsFile .7z)
#      mkdir -p $stripped
#      7z x  $file  -o${stripped}
#     ;;
#   *.rar)
#      stripped=$(basename $hdfsFile .rar)
#      mkdir -p $stripped
#      7z x  $file  -o${stripped}
#     ;;
#   *.zip)
#      stripped=$(basename $hdfsFile .zip)
#      mkdir -p $stripped
#      unzip $file -d ./$stripped
#     ;;
#   *)
#     help
#     ;;
# esac

   ${HADOOP_DIR}/bin/hdfs dfs -copyFromLocal ./$stripped $dir
   ${HADOOP_DIR}/bin/hdfs dfs -chown -R ${HDFS_USER} $dir

# State Machine State Change. File now copied to HDFS.
# Java program checks if this file is written to see if this directory exists:  /Projects/PROJ/Dataset/$(basename $hdfsfile)

   rm $file
   rm -rf ./$stripped

# State Machine State Change. File is now cleaned up from local staging directory
# Java program checks if this file is written to see if this directory does not exist

   
echo "Done copying from $file to $dir"
